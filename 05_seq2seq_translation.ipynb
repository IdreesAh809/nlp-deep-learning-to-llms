{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3597dc01-c15c-4b0d-b0e6-bbb68fade6ce",
   "metadata": {},
   "source": [
    "# Seq2Seq Machine Translation (English → Portuguese)\n",
    "\n",
    "In this notebook, we implement a **Sequence-to-Sequence (Seq2Seq)** model with Attention for translating **English sentences into Portuguese**.\n",
    "\n",
    "### Why Seq2Seq?\n",
    "- Unlike plain RNN/LSTM/GRU, Seq2Seq is designed for **input and output sequences of different lengths**.\n",
    "- Example: An English sentence may have 5 words, while the Portuguese translation may have 8 words.\n",
    "\n",
    "### Key Idea\n",
    "- **Encoder** reads the input (English) and converts it into context vectors.\n",
    "- **Decoder** generates the output (Portuguese), word by word, using **Attention** to focus on relevant input words.\n",
    "\n",
    "### Applications\n",
    "- Machine Translation (Google Translate, etc.)\n",
    "- Text Summarization\n",
    "- Chatbots\n",
    "- Speech-to-Text systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c48db95-97c3-43c5-902a-3ac34dc552ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries & load dataset\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "examples, metadata = tfds.load(\n",
    "    'ted_hrlr_translate/pt_to_en',\n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "# Use small sample (300 train, 50 val) for fast testing\n",
    "train_examples = examples['train'].take(300)\n",
    "val_examples   = examples['validation'].take(50)\n",
    "\n",
    "print(\"✅ Dataset loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad743fc-9673-47f5-ab42-10c29ab1127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese vocab size: 1032\n",
      "English vocab size: 969\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize\n",
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=1000)\n",
    "\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=1000)\n",
    "\n",
    "print(\"Portuguese vocab size:\", tokenizer_pt.vocab_size)\n",
    "print(\"English vocab size:\", tokenizer_en.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056648bd-3e2c-47d3-a3f9-a9db04d3addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset prepared\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Encode and batch\n",
    "MAX_LEN = 20\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def encode(pt, en):\n",
    "    pt = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(pt.numpy())[:MAX_LEN] + [tokenizer_pt.vocab_size+1]\n",
    "    en = [tokenizer_en.vocab_size] + tokenizer_en.encode(en.numpy())[:MAX_LEN] + [tokenizer_en.vocab_size+1]\n",
    "    return pt, en\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "\n",
    "train_dataset = (train_examples.map(tf_encode)\n",
    "                 .padded_batch(BATCH_SIZE, padded_shapes=([None], [None])))\n",
    "\n",
    "val_dataset = (val_examples.map(tf_encode)\n",
    "               .padded_batch(BATCH_SIZE, padded_shapes=([None], [None])))\n",
    "\n",
    "print(\"✅ Dataset prepared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b792140e-c9e1-46d1-a44c-ca6e6f934bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size+2, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def init_hidden(self, batch_sz):\n",
    "        return tf.zeros((batch_sz, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edce4c7-9f17-4dde-97d0-6195f73fb36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define Decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size+2, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size+2)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        x = self.fc(output)\n",
    "        return x, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2e4226-9374-4cb0-ab38-e5a67caac1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Initialize models\n",
    "embedding_dim = 32\n",
    "units = 128\n",
    "\n",
    "encoder = Encoder(tokenizer_pt.vocab_size, embedding_dim, units)\n",
    "decoder = Decoder(tokenizer_en.vocab_size, embedding_dim, units)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10337b76-db50-4008-87e8-c8823c60aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define training step\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([tokenizer_en.vocab_size] * inp.shape[0], 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_object(targ[:, t], predictions[:, -1, :])\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1739e24e-d32d-49c1-a7fc-3599f5bd782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.4513\n",
      "Epoch 2, Loss: 5.2274\n",
      "Epoch 3, Loss: 4.9889\n",
      "Epoch 4, Loss: 4.8332\n",
      "Epoch 5, Loss: 4.6881\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train (only 2 epochs, small dataset)\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(20)):\n",
    "        enc_hidden = encoder.init_hidden(inp.shape[0])\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/(batch+1):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83cf5723-8f59-4981-9f5a-3b27ff8b4a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Example: and and  , \n"
     ]
    }
   ],
   "source": [
    "# Step 9: Test translation\n",
    "def translate(sentence):\n",
    "    inputs = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(sentence)[:MAX_LEN] + [tokenizer_pt.vocab_size+1]\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "\n",
    "    result = []\n",
    "    hidden = encoder.init_hidden(1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer_en.vocab_size], 0)\n",
    "\n",
    "    for t in range(MAX_LEN):\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0, -1, :]).numpy()\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            break\n",
    "        result.append(predicted_id)\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return tokenizer_en.decode(result)\n",
    "\n",
    "print(\"✅ Example:\", translate(\"olá mundo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5790d369-fd7a-48a2-8459-d490443e204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT: olá\n",
      "EN:                                                  \n",
      "\n",
      "PT: como você está?\n",
      "EN: and  and   ,   ,   ,   \n",
      "\n",
      "PT: bom dia\n",
      "EN: and                                                  \n",
      "\n",
      "PT: eu gosto de aprender\n",
      "EN: and  and   ,   ,   ,   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "    # Encode the Portuguese sentence\n",
    "    inputs = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(sentence) + [tokenizer_pt.vocab_size + 1]\n",
    "    inputs = tf.expand_dims(inputs, 0)  # batch size = 1\n",
    "\n",
    "    result = []\n",
    "\n",
    "    hidden = tf.zeros((1, units))  # batch size = 1\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer_en.vocab_size], 0)  # start token\n",
    "\n",
    "    for t in range(50):\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        # Remove batch dimension\n",
    "        predictions = tf.squeeze(predictions, axis=0)  # shape -> (seq_len, vocab_size)\n",
    "        predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]  # first token\n",
    "\n",
    "        if predicted_id == tokenizer_en.vocab_size + 1:  # end token\n",
    "            break\n",
    "\n",
    "        result.append(tokenizer_en.decode([predicted_id]))\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "# Test\n",
    "test_sentences = [\n",
    "    \"olá\",\n",
    "    \"como você está?\",\n",
    "    \"bom dia\",\n",
    "    \"eu gosto de aprender\"\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    print(f\"PT: {sent}\")\n",
    "    print(f\"EN: {translate(sent)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a6e40-d37e-43e2-960b-972d94afb384",
   "metadata": {},
   "source": [
    "## Dataset and Training Overview\n",
    "\n",
    "- **Dataset Size:** 300–500 sentences (Portuguese ↔ English)\n",
    "- **Purpose:** Learn Seq2Seq with Attention quickly\n",
    "- **Training:** Few epochs (5) for fast demonstration\n",
    "- **What We Learn:** \n",
    "  - Tokenization & embeddings\n",
    "  - Encoder-Decoder structure\n",
    "  - Attention mechanism\n",
    "  - Sample translations & loss reduction\n",
    "- **Note:** Small dataset & few epochs → faster training, not production-ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74219261-0e18-41a1-8df8-911a75ec0082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
